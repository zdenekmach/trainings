# Copilot & Databricks - PraktickÃ¡ CviÄenÃ­

## Jak pracovat s cviÄenÃ­mi

**Pro studenty:**
- NejdÅ™Ã­v zkus sÃ¡m - uÄÃ­Å¡ se nejvÃ­c z vlastnÃ­ch pokusÅ¯ a chyb
- Prompty piÅ¡ v ÄeÅ¡tinÄ› i angliÄtinÄ› - AI rozumÃ­ obÄ›ma
- Pokud tÃ¡peÅ¡ 10+ minut, koukni na nÃ¡povÄ›du
- Srovnej svoje Å™eÅ¡enÃ­ s uvedenÃ½m - mÅ¯Å¾e bÃ½t jinÃ© a stejnÄ› sprÃ¡vnÃ©
- VÅ½DY validuj vÃ½sledky - spusÅ¥ dotaz a zkontroluj jestli dÃ¡vÃ¡ smysl

**Pro lektory:**
- NespÄ›chej, dej Äas na experimentovÃ¡nÃ­
- Ptej se "Co jsi zkouÅ¡el?" ne "NevÃ­Å¡ to?"
- RÅ¯znÃ© pÅ™Ã­stupy jsou OK pokud fungujÃ­
- ChoÄ mezi lidmi bÄ›hem hands-on, pomÃ¡hej proaktivnÄ›

---

## CviÄenÃ­ 1: Transformace VÃ¡gnÃ­ho Promptu ğŸŸ¢

### ZadÃ¡nÃ­

MÃ¡Å¡ tyto vÃ¡gnÃ­ prompty. PÅ™epiÅ¡ je na dobrÃ© prompty pouÅ¾itÃ­m 4 komponent: **Role + Kontext + Ãškol + FormÃ¡t**.

**Prompt A:** "UkaÅ¾ mi zajÃ­mavÃ© vÄ›ci o zÃ¡kaznÃ­cÃ­ch"

**Prompt B:** "Najdi problÃ©movÃ© produkty"

**Prompt C:** "Analyzuj trendy"

**CÃ­l:** ProcviÄit anatomii dobrÃ©ho promptu

**ÄŒasovÃ½ odhad:** 15 minut

### Co budeÅ¡ potÅ™ebovat
- PapÃ­r a tuÅ¾ka (nebo textovÃ½ editor)
- PÅ™edstav si Å¾e mÃ¡Å¡ tyto tabulky:
  - `customers` (customer_id, age_group, region, registration_date)
  - `orders` (order_id, customer_id, product_id, order_value, order_date)
  - `products` (product_id, product_name, category, price)

### OÄekÃ¡vanÃ½ vÃ½stup
Pro kaÅ¾dÃ½ prompt napiÅ¡:
1. ROLE (kdo je AI)
2. KONTEXT (jakÃ¡ data mÃ¡Å¡)
3. ÃšKOL (co pÅ™esnÄ› chceÅ¡)
4. FORMÃT (jak to chceÅ¡ dostat)

---

### NÃ¡povÄ›da k CviÄenÃ­ 1

<details>
<summary>Klikni pro nÃ¡povÄ›du</summary>

**Hint 1:** Co znamenÃ¡ "zajÃ­mavÃ© vÄ›ci"? BuÄ konkrÃ©tnÃ­ - segmentace? TOP zÃ¡kaznÃ­ci? Churn risk?

**Hint 2:** "ProblÃ©movÃ© produkty" = nÃ­zkÃ© prodeje? ZÃ¡pornÃ© reviews? VrÃ¡cenÃ© zboÅ¾Ã­? Specifikuj!

**Hint 3:** "Analyzuj trendy" - jakÃ© trendy? ÄŒasovÃ©? GeografickÃ©? ProduktovÃ©?

**Template:**
```
Kontext: MÃ¡m tabulky @[nÃ¡zvy] s columns: [vÃ½Äet relevantnÃ­ch sloupcÅ¯]
Ãškol: [SpecifickÃ¡ akce s mÄ›Å™itelnÃ½mi parametry]
FormÃ¡t: SQL dotaz / Python kÃ³d / Viz + vÃ½stupnÃ­ columns
```

</details>

---

### Å˜eÅ¡enÃ­ CviÄenÃ­ 1

**Prompt A - PÅ˜ED:**
```
"UkaÅ¾ mi zajÃ­mavÃ© vÄ›ci o zÃ¡kaznÃ­cÃ­ch"
```

**Prompt A - PO:**
```
Role: Jsi expert na customer analytics pro e-commerce.

Kontext: MÃ¡m tabulky @customers (customer_id, age_group, region, 
registration_date) a @orders (order_id, customer_id, order_value, order_date).

Ãškol: Segmentuj zÃ¡kaznÃ­ky do 3 skupin podle prÅ¯mÄ›rnÃ© order_value:
- Low spenders (0-500 KÄ avg)
- Mid spenders (500-2000 KÄ avg)
- High spenders (2000+ KÄ avg)

Pro kaÅ¾dÃ½ segment urÄi:
- PoÄet zÃ¡kaznÃ­kÅ¯ (absolutnÃ­ + %)
- TOP region (kde jich je nejvÃ­c)
- PrÅ¯mÄ›rnÃ¡ age_group

FormÃ¡t: SQL dotaz + tabulka se segmenty + 2-3 key business insights
```

**ProÄ je to lepÅ¡Ã­:**
- "ZajÃ­mavÃ© vÄ›ci" â†’ KonkrÃ©tnÃ­ segmentace s ÄÃ­sly
- AI vÃ­ pÅ™esnÄ› jakÃ© sloupce pouÅ¾Ã­t
- JasnÃ½ output format

---

**Prompt B - PÅ˜ED:**
```
"Najdi problÃ©movÃ© produkty"
```

**Prompt B - PO:**
```
Kontext: @products (product_id, product_name, category, price) a 
@orders (order_date, product_id, quantity_sold, revenue).

Ãškol: Identifikuj "problÃ©movÃ©" produkty podle tÄ›chto kritÃ©riÃ­:
1. Prodeje klesly o 30%+ v poslednÃ­ch 3 mÄ›sÃ­cÃ­ch vs. pÅ™edchozÃ­ 3 mÄ›sÃ­ce
2. NEBO celkovÃ½ revenue < 10,000 KÄ za poslednÃ­ quarter
3. NEBO quantity_sold < 10 kusÅ¯ za poslednÃ­ mÄ›sÃ­c

VÃ½stup: TOP 20 problÃ©movÃ½ch produktÅ¯ seÅ™azenÃ½ch podle % poklesu.
Zobraz: product_name, category, revenue_current_quarter, revenue_prev_quarter,
percent_change, quantity_last_month.

FormÃ¡t: SQL + seÅ™aÄ DESC podle abs(percent_change)
```

**ProÄ je to lepÅ¡Ã­:**
- "ProblÃ©movÃ©" mÃ¡ 3 jasnÃ© definice s ÄÃ­sly
- AI vÃ­ jak poÄÃ­tat percent_change
- KonkrÃ©tnÃ­ timeframes

---

**Prompt C - PÅ˜ED:**
```
"Analyzuj trendy"
```

**Prompt C - PO:**
```
Kontext: @orders (order_date, product_category, revenue_czk) za poslednÃ­ch 24 mÄ›sÃ­cÅ¯.

Ãškol: Time series analÃ½za trendÅ¯ revenue podle product_category.
1. Agreguj revenue po mÄ›sÃ­cÃ­ch pro kaÅ¾dou kategorii
2. VypoÄÃ­tej month-over-month % change
3. Identifikuj TOP 3 fastest-growing categories (avg monthly growth)
4. Identifikuj categories s declining trendem (negative growth 3+ mÄ›sÃ­ce po sobÄ›)

VÃ½stup: 
- SQL dotazy pro aggregaci
- Multi-line chart (1 line per category)
- Tabulka s TOP 3 growing a declining categories + growth rates

FormÃ¡t: Python (pandas) pro vÃ½poÄty + matplotlib pro viz
```

**ProÄ je to lepÅ¡Ã­:**
- "Trendy" â†’ ÄŒasovÃ© Å™ady, konkrÃ©tnÃ­ metriky (MoM change)
- DefinovÃ¡no co znamenÃ¡ "growing" a "declining"
- JasnÃ½ visualization format

---

**ÄŒastÃ© chyby:**
- âŒ StÃ¡le vÃ¡gnÃ­: "Najdi zÃ¡kaznÃ­ky co hodnÄ› nakupujÃ­" (kolik je "hodnÄ›"?)
- âœ… KonkrÃ©tnÃ­: "TOP 50 zÃ¡kaznÃ­kÅ¯ podle celkovÃ©ho revenue za 2024"

- âŒ Bez kontextu: AI hÃ¡dÃ¡ nÃ¡zvy tabulek a sloupcÅ¯
- âœ… S kontextem: @ mention nebo explicit vÃ½Äet sloupcÅ¯

---

## CviÄenÃ­ 2: Prompt Engineering v Databricks ğŸŸ¡

### ZadÃ¡nÃ­

V Databricks notebooku vytvoÅ™ analÃ½zu prodejÅ¯ Ibuprofenu podle mÄ›stskÃ½ch ÄÃ¡stÃ­ Prahy za Q4 2024.

**PouÅ¾ij prompt chaining - rozdÄ›l na 3 kroky:**

**Krok 1:** Exploration - zjisti schÃ©ma a sample data

**Krok 2:** AnalÃ½za - TOP 10 mÄ›stskÃ½ch ÄÃ¡stÃ­ podle revenue

**Krok 3:** Vizualizace - horizontal bar chart

**CÃ­l:** ProcviÄit prompt chaining a @ mentions

**ÄŒasovÃ½ odhad:** 25 minut

### Co budeÅ¡ potÅ™ebovat
- Databricks notebook
- Demo tabulka: `@demo_sales_data`
  - Columns: product_name, city_district, sale_date, revenue_czk, quantity_sold

### OÄekÃ¡vanÃ½ vÃ½stup
- 3 buÅˆky v notebooku (1 prompt per step)
- SQL dotazy s vÃ½sledky
- Bar chart vizualizace

### TestovacÃ­ data
```
PÅ™edpoklÃ¡dej Å¾e demo_sales_data obsahuje:
- Products: "Ibuprofen 400mg", "Aspirin 500mg", "Paracetamol 500mg", ...
- Districts: "Praha 1", "Praha 2", ... "Praha 10", "Vinohrady", "Å½iÅ¾kov", ...
- Dates: 2024-01-01 aÅ¾ 2024-12-31
```

---

### NÃ¡povÄ›da k CviÄenÃ­ 2

<details>
<summary>Klikni pro nÃ¡povÄ›du</summary>

**Hint 1 - Krok 1 (Exploration):**
```
Prompt: "Z @demo_sales_data ukaÅ¾ mi:
1. SchÃ©ma tabulky (column names a datatypes)
2. PrvnÃ­ch 5 Å™Ã¡dkÅ¯ kde product_name LIKE '%Ibuprofen%'
3. Count total rows"
```

**Hint 2 - Krok 2 (AnalÃ½za):**
```
Prompt: "Z @demo_sales_data pro produkt 'Ibuprofen 400mg':
- Filtruj sale_date mezi 2024-10-01 a 2024-12-31 (Q4)
- Filtruj city_district kde zaÄÃ­nÃ¡ 'Praha' (jen Praha, ne okolÃ­)
- Agreguj podle city_district: SUM(revenue_czk), SUM(quantity_sold)
- TOP 10 podle revenue DESC
SQL dotaz s columns: city_district, total_revenue, total_quantity"
```

**Hint 3 - Krok 3 (Viz):**
```
Prompt: "Z pÅ™edchozÃ­ch vÃ½sledkÅ¯ vytvoÅ™ horizontal bar chart:
- X-axis: total_revenue (v mil. KÄ)
- Y-axis: city_district (seÅ™aÄ od nejvyÅ¡Å¡Ã­ho revenue)
- Color: single color (modrÃ¡)
- Title: 'TOP 10 mÄ›stskÃ½ch ÄÃ¡stÃ­ - Ibuprofen 400mg Q4 2024'
- Zobraz hodnoty na konci barÅ¯"
```

</details>

---

### Å˜eÅ¡enÃ­ CviÄenÃ­ 2

**Krok 1: Exploration**

Prompt v Databricks:
```
Z @demo_sales_data ukaÅ¾ mi:
1. SchÃ©ma tabulky (DESCRIBE TABLE)
2. Sample 5 Å™Ã¡dkÅ¯ pro produkt obsahujÃ­cÃ­ "Ibuprofen"
3. CelkovÃ½ poÄet Å™Ã¡dkÅ¯

VÃ½stup: SQL dotazy pro kaÅ¾dÃ½ bod
```

AI vygeneruje:
```sql
-- 1. SchÃ©ma
DESCRIBE TABLE demo_sales_data;

-- 2. Sample
SELECT * FROM demo_sales_data
WHERE product_name LIKE '%Ibuprofen%'
LIMIT 5;

-- 3. Count
SELECT COUNT(*) as total_rows FROM demo_sales_data;
```

**Validace:** Zkontroluj Å¾e vidÃ­Å¡ sprÃ¡vnÃ© sloupce a data vypadajÃ­ reÃ¡lnÄ›.

---

**Krok 2: TOP 10 AnalÃ½za**

Prompt:
```
Z @demo_sales_data najdi TOP 10 mÄ›stskÃ½ch ÄÃ¡stÃ­ Prahy podle revenue 
pro produkt "Ibuprofen 400mg" v Q4 2024 (Å™Ã­jen-prosinec).

Filtr:
- product_name = 'Ibuprofen 400mg'
- sale_date >= '2024-10-01' AND sale_date < '2025-01-01'
- city_district LIKE 'Praha%'

Agregace podle city_district:
- total_revenue = SUM(revenue_czk)
- total_quantity = SUM(quantity_sold)
- avg_price_per_unit = total_revenue / total_quantity

SeÅ™aÄ DESC podle total_revenue, TOP 10.

VÃ½stup: SQL dotaz s komentÃ¡Å™i
```

AI vygeneruje:
```sql
-- TOP 10 mÄ›stskÃ½ch ÄÃ¡stÃ­ Prahy - Ibuprofen 400mg Q4 2024
SELECT
    city_district,
    SUM(revenue_czk) as total_revenue,
    SUM(quantity_sold) as total_quantity,
    SUM(revenue_czk) / NULLIF(SUM(quantity_sold), 0) as avg_price_per_unit
FROM demo_sales_data
WHERE
    product_name = 'Ibuprofen 400mg'
    AND sale_date >= '2024-10-01'
    AND sale_date < '2025-01-01'
    AND city_district LIKE 'Praha%'
GROUP BY city_district
ORDER BY total_revenue DESC
LIMIT 10;
```

**Validace:**
- SpusÅ¥ LIMIT 10 a check ÄÃ­selnÃ© hodnoty
- DÃ¡vÃ¡ smysl Å¾e Praha 1 mÃ¡ vyÅ¡Å¡Ã­ revenue neÅ¾ Praha 10? (depends on demographics)
- Zkontroluj Å¾e avg_price_per_unit je rozumnÃ© (napÅ™. 50-150 KÄ)

---

**Krok 3: Vizualizace**

Prompt:
```
Z vÃ½sledkÅ¯ pÅ™edchozÃ­ho dotazu vytvoÅ™ horizontal bar chart:

Config:
- Chart type: horizontal bar
- X-axis: total_revenue (label: "Revenue (KÄ)")
- Y-axis: city_district (seÅ™azenÃ½ od nejvyÅ¡Å¡Ã­ho revenue)
- Color: modrÃ¡ (#1f77b4)
- Title: "TOP 10 mÄ›stskÃ½ch ÄÃ¡stÃ­ - Ibuprofen 400mg Q4 2024"
- Zobraz hodnoty na konci kaÅ¾dÃ©ho baru (formatted s tisÃ­ci: "1,234,567 KÄ")

Python matplotlib nebo Databricks viz.
```

AI vygeneruje Python:
```python
import matplotlib.pyplot as plt
import pandas as pd

# Data z pÅ™edchozÃ­ho SQL dotazu
df = spark.sql("""
    SELECT city_district, total_revenue
    FROM (...previous query...)
""").toPandas()

# Sort DESC
df = df.sort_values('total_revenue', ascending=True)  # True pro horizontal

# Plot
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.barh(df['city_district'], df['total_revenue'], color='#1f77b4')

# Labels on bars
for bar in bars:
    width = bar.get_width()
    ax.text(width, bar.get_y() + bar.get_height()/2,
            f'{width:,.0f} KÄ',
            ha='left', va='center', fontsize=9)

ax.set_xlabel('Revenue (KÄ)', fontsize=12)
ax.set_title('TOP 10 mÄ›stskÃ½ch ÄÃ¡stÃ­ - Ibuprofen 400mg Q4 2024', fontsize=14)
plt.tight_layout()
plt.show()
```

**Validace:**
- Chart se zobrazil?
- PoÅ™adÃ­ je sprÃ¡vnÄ› (highest nahoÅ™e nebo dole, depends on sort)
- Labels jsou readable?

---

**AlternativnÃ­ pÅ™Ã­stup - Databricks native viz:**
```python
# JednoduÅ¡Å¡Ã­ - pouÅ¾ij Databricks display()
display(df)
# Pak v UI: switch to "Visualization" â†’ "Bar chart" â†’ configure
```

**ÄŒastÃ© chyby:**
- âŒ ZapomenutÃ½ WHERE filter na Q4 â†’ vracÃ­ celÃ½ rok
- âŒ LIKE '%Praha' mÃ­sto 'Praha%' â†’ nehledÃ¡ sprÃ¡vnÄ›
- âŒ NeoÅ¡etÅ™enÃ© NULL values v quantity â†’ division by zero

---

## CviÄenÃ­ 3: Customer Segmentation (RFM) ğŸŸ¡

### ZadÃ¡nÃ­

VytvoÅ™ customer segmentation pomocÃ­ RFM analÃ½zy (Recency, Frequency, Monetary) pro produktovou kategorii "Pain Relief".

**RFM definice:**
- **Recency:** Kolik dnÃ­ od poslednÃ­ objednÃ¡vky
- **Frequency:** PoÄet objednÃ¡vek za poslednÃ­ch 12 mÄ›sÃ­cÅ¯
- **Monetary:** CelkovÃ¡ hodnota objednÃ¡vek za 12 mÄ›sÃ­cÅ¯

**Segmenty:**
1. Champions: R=high, F=high, M=high
2. Loyal Customers: F=high, M=high
3. At Risk: R=low (dlouho nenakupovali)
4. Lost: R=very low

**CÃ­l:** ProcviÄit JOINy, agregace, segmentaci

**ÄŒasovÃ½ odhad:** 40 minut

### Co budeÅ¡ potÅ™ebovat
- `@demo_customers` (customer_id, age_group, region)
- `@demo_orders` (order_id, customer_id, product_category, order_value, order_date)

### OÄekÃ¡vanÃ½ vÃ½stup
1. SQL dotaz pro RFM vÃ½poÄet
2. SegmentaÄnÃ­ logika (CASE WHEN)
3. Tabulka: segment_name, customer_count, avg_monetary, top_region
4. Business recommendations pro kaÅ¾dÃ½ segment

---

### NÃ¡povÄ›da k CviÄenÃ­ 3

<details>
<summary>NÃ¡povÄ›da - strategickÃ½ pÅ™Ã­stup</summary>

**RozloÅ¾ na kroky:**

1. **Filtruj data:** Jen Pain Relief, poslednÃ­ch 12 mÄ›sÃ­cÅ¯
2. **VypoÄÃ­tej RFM metriky pro kaÅ¾dÃ©ho zÃ¡kaznÃ­ka:**
   - Recency = DATEDIFF(CURRENT_DATE, MAX(order_date))
   - Frequency = COUNT(DISTINCT order_id)
   - Monetary = SUM(order_value)
3. **Segmentuj:** CASE WHEN logic na zÃ¡kladÄ› RFM
4. **Agreguj podle segmentu:** count, averages, top region

**Na co si dÃ¡t pozor:**
- CURRENT_DATE vs. fixed date (pro reproducible results pouÅ¾ij napÅ™. '2024-12-31')
- LEFT JOIN vs. INNER JOIN (chceÅ¡ jen customers co majÃ­ orders)
- DISTINCT order_id aby duplicates nepoÄÃ­tal 2x

</details>

---

### Å˜eÅ¡enÃ­ CviÄenÃ­ 3

**Krok 1: RFM Calculation**

```sql
-- RFM AnalÃ½za pro Pain Relief kategorii
WITH customer_rfm AS (
    SELECT
        c.customer_id,
        c.age_group,
        c.region,
        -- Recency (dny od poslednÃ­ objednÃ¡vky)
        DATEDIFF('2024-12-31', MAX(o.order_date)) as recency_days,
        -- Frequency (poÄet objednÃ¡vek za 12 mÄ›sÃ­cÅ¯)
        COUNT(DISTINCT o.order_id) as frequency,
        -- Monetary (celkovÃ¡ hodnota)
        SUM(o.order_value) as monetary_value
    FROM demo_customers c
    INNER JOIN demo_orders o ON c.customer_id = o.customer_id
    WHERE
        o.product_category = 'Pain Relief'
        AND o.order_date >= DATE_SUB('2024-12-31', 365)  -- PoslednÃ­ch 12 mÄ›sÃ­cÅ¯
        AND o.order_date <= '2024-12-31'
    GROUP BY c.customer_id, c.age_group, c.region
)
SELECT * FROM customer_rfm LIMIT 10;
```

**Krok 2: Segmentace**

```sql
WITH customer_rfm AS (...previous CTE...),

customer_segments AS (
    SELECT
        *,
        CASE
            -- Champions: recently bought, often, high value
            WHEN recency_days <= 30 AND frequency >= 5 AND monetary_value >= 5000
                THEN 'Champions'
            
            -- Loyal Customers: frequent + high value (ale moÅ¾nÃ¡ ne recent)
            WHEN frequency >= 4 AND monetary_value >= 3000
                THEN 'Loyal Customers'
            
            -- Potential Loyalists: recent, decent frequency
            WHEN recency_days <= 60 AND frequency >= 2 AND monetary_value >= 1000
                THEN 'Potential Loyalists'
            
            -- At Risk: dlouho nenakoupili, ale historically good
            WHEN recency_days > 90 AND recency_days <= 180 AND frequency >= 3
                THEN 'At Risk'
            
            -- Lost: velmi dlouho nenakoupili
            WHEN recency_days > 180
                THEN 'Lost'
            
            -- New Customers: nÃ­zkÃ¡ frequency
            WHEN frequency <= 1
                THEN 'New Customers'
            
            ELSE 'Regular'
        END as segment
    FROM customer_rfm
)
SELECT * FROM customer_segments LIMIT 10;
```

**Krok 3: Segment Profiling**

```sql
WITH (...previous CTEs...),

segment_summary AS (
    SELECT
        segment,
        COUNT(*) as customer_count,
        ROUND(AVG(recency_days), 1) as avg_recency,
        ROUND(AVG(frequency), 1) as avg_frequency,
        ROUND(AVG(monetary_value), 0) as avg_monetary,
        -- TOP region per segment
        FIRST_VALUE(region) OVER (
            PARTITION BY segment
            ORDER BY COUNT(*) OVER (PARTITION BY segment, region) DESC
        ) as top_region
    FROM customer_segments
    GROUP BY segment, region
)
SELECT
    segment,
    customer_count,
    ROUND(100.0 * customer_count / SUM(customer_count) OVER (), 1) as pct_of_total,
    avg_recency,
    avg_frequency,
    avg_monetary,
    top_region
FROM segment_summary
GROUP BY segment, customer_count, avg_recency, avg_frequency, avg_monetary, top_region
ORDER BY customer_count DESC;
```

**VÃ½stup:**
| segment | customer_count | pct_of_total | avg_recency | avg_frequency | avg_monetary | top_region |
|---------|----------------|--------------|-------------|---------------|--------------|------------|
| Regular | 450 | 35.2 | 65.3 | 2.8 | 2,150 | Praha |
| Loyal Customers | 280 | 21.9 | 45.1 | 6.2 | 4,800 | Brno |
| Champions | 120 | 9.4 | 15.2 | 8.5 | 7,200 | Praha |
| At Risk | 95 | 7.4 | 125.7 | 4.1 | 3,500 | Ostrava |
| Lost | 180 | 14.1 | 245.3 | 3.2 | 2,900 | Praha |
| ... | ... | ... | ... | ... | ... | ... |

**Krok 4: Business Recommendations**

```
Champions (9.4%):
ğŸ’ NejvhodnÄ›jÅ¡Ã­ pro: VIP program, early access k novÃ½m produktÅ¯m, referral rewards
ğŸ“Š PrÅ¯mÄ›rnÃ¡ hodnota: 7,200 KÄ - udrÅ¾uj je happy

Loyal Customers (21.9%):
ğŸ¯ Strategie: Upsell/cross-sell, membership programy
ğŸ’¡ Opportunity: PÅ™evÃ©st na Champions (zvÃ½Å¡it frequency)

At Risk (7.4%):
âš ï¸ Akce: Win-back kampaÅˆ, speciÃ¡lnÃ­ nabÃ­dky, survey proÄ nenak upovali
â° Urgence: Jednaj RYCHLE, jinak pÅ™ejdou do Lost

Lost (14.1%):
ğŸ˜” Realita: NÃ¡kladnÃ© zÃ­skat zpÃ¡tky, moÅ¾nÃ¡ jiÅ¾ u konkurence
ğŸ¤” RozhodnutÃ­: Aggressive win-back vs. let go

New Customers (X%):
ğŸ‰ Focus: Onboarding, education o produktech, early loyalty incentives
ğŸš€ Goal: PÅ™evÃ©st na Regular â†’ Loyal

Regular (35.2%):
ğŸ“ˆ Opportunity: Largest segment - zvÃ½Å¡it frequency a monetary
ğŸ’¡ Tactics: PersonalizovanÃ© recommendations, bundle deals
```

---

**ÄŒastÃ© chyby:**
- âŒ PoÄÃ­tÃ¡nÃ­ vÅ¡ech orders mÃ­sto DISTINCT â†’ inflated frequency
- âŒ ZapomenutÃ½ filter na category â†’ RFM pro vÅ¡echny produkty
- âŒ Hardcoded thresholdy bez znalosti dat â†’ nesmyslnÃ© segmenty (vÅ¡ichni v jednom)

**AlternativnÃ­ pÅ™Ã­stup - Percentily:**
```sql
-- MÃ­sto hardcoded thresholds pouÅ¾ij percentily
WITH rfm_scores AS (
    SELECT *,
        NTILE(5) OVER (ORDER BY recency_days DESC) as R_score,  -- DESC: lower recency = higher score
        NTILE(5) OVER (ORDER BY frequency ASC) as F_score,
        NTILE(5) OVER (ORDER BY monetary_value ASC) as M_score
    FROM customer_rfm
)
SELECT *,
    CASE
        WHEN R_score >= 4 AND F_score >= 4 AND M_score >= 4 THEN 'Champions'
        WHEN F_score >= 4 AND M_score >= 4 THEN 'Loyal'
        ...
    END as segment
FROM rfm_scores;
```
â†’ AdaptabilnÃ­, funguje i kdyÅ¾ data scale zmÄ›nÃ­

---

## CviÄenÃ­ 4: Debugging - Najdi Chybu v AI KÃ³du ğŸŸ¢

### ZadÃ¡nÃ­

AI ti vygeneroval tento SQL pro "prÅ¯mÄ›rnÃ¡ cena produktu podle regionu". Najdi chybu a opravo ji.

```sql
-- Prompt byl: "prÅ¯mÄ›rnÃ¡ cena produktu podle regionu"
SELECT
    region,
    AVG(price) as avg_price
FROM (
    SELECT
        region,
        product,
        AVG(unit_price) as price
    FROM sales
    GROUP BY region, product
) AS subquery
GROUP BY region
ORDER BY avg_price DESC;
```

**OtÃ¡zky:**
1. Co je Å¡patnÄ›?
2. Jak to opravit?
3. ProÄ AI udÄ›lal tuto chybu?

**CÃ­l:** ProcviÄit validaci AI vÃ½stupÅ¯

**ÄŒasovÃ½ odhad:** 10 minut

---

### NÃ¡povÄ›da k CviÄenÃ­ 4

<details>
<summary>Hint</summary>

**Hint 1:** PodÃ­vej se na inner query. Co poÄÃ­tÃ¡? PrÅ¯mÄ›rnou cenu pro kaÅ¾dÃ½ produkt v regionu.

**Hint 2:** Outer query pak poÄÃ­tÃ¡ prÅ¯mÄ›r tÄ›chto prÅ¯mÄ›rÅ¯. Je to sprÃ¡vnÄ›?

**Hint 3:** PÅ™Ã­klad proÄ je to Å¡patnÄ›:
- Region A: Product 1 (10 sales, avg 100 KÄ), Product 2 (1000 sales, avg 50 KÄ)
- Å patnÄ›: (100 + 50) / 2 = 75 KÄ
- SprÃ¡vnÄ›: VÃ¡Å¾enÃ½ prÅ¯mÄ›r = ~52 KÄ (protoÅ¾e Product 2 mÃ¡ 100x vÃ­c sales)

</details>

---

### Å˜eÅ¡enÃ­ CviÄenÃ­ 4

**Co je Å¡patnÄ›:**

Toto je **average-of-averages problÃ©m**!

Inner query poÄÃ­tÃ¡ prÅ¯mÄ›rnou cenu pro KAÅ½DÃ produkt v regionu. Outer query pak poÄÃ­tÃ¡ prÅ¯mÄ›r tÄ›chto prÅ¯mÄ›rÅ¯, coÅ¾ NErespektuje poÄet transakcÃ­.

**ProÄ je to problÃ©m:**

PÅ™edstav si Region A:
- Product 1: 10 prodejÅ¯ po 100 KÄ (avg = 100 KÄ)
- Product 2: 1000 prodejÅ¯ po 50 KÄ (avg = 50 KÄ)

**Å patnÃ½ vÃ½poÄet** (average-of-averages):
```
avg_price = (100 + 50) / 2 = 75 KÄ
```
â†’ DÃ¡vÃ¡ Product 1 a Product 2 stejnou vÃ¡hu, i kdyÅ¾ Product 2 mÃ¡ 100x vÃ­ce transakcÃ­!

**SprÃ¡vnÃ½ vÃ½poÄet** (vÃ¡Å¾enÃ½ prÅ¯mÄ›r):
```
Total revenue = (10 Ã— 100) + (1000 Ã— 50) = 1,000 + 50,000 = 51,000 KÄ
Total quantity = 10 + 1000 = 1010
avg_price = 51,000 / 1010 = ~50.5 KÄ
```

---

**SprÃ¡vnÃ© Å™eÅ¡enÃ­ - Varianta 1 (jednoduchÃ¡):**

```sql
-- JednoduchÃ½ prÅ¯mÄ›r pÅ™Ã­mo z raw data
SELECT
    region,
    AVG(unit_price) as avg_price
FROM sales
GROUP BY region
ORDER BY avg_price DESC;
```

**ProÄ funguje:** PoÄÃ­tÃ¡ prÅ¯mÄ›r ze vÅ¡ech transakcÃ­, ne z agregovanÃ½ch prÅ¯mÄ›rÅ¯.

---

**SprÃ¡vnÃ© Å™eÅ¡enÃ­ - Varianta 2 (vÃ¡Å¾enÃ½ prÅ¯mÄ›r):**

Pokud chceÅ¡ weighted average podle quantity:

```sql
SELECT
    region,
    SUM(unit_price * quantity) / SUM(quantity) as weighted_avg_price,
    SUM(quantity) as total_quantity,
    COUNT(*) as total_transactions
FROM sales
GROUP BY region
ORDER BY weighted_avg_price DESC;
```

**ProÄ funguje:** VÃ¡Å¾Ã­ kaÅ¾dou cenu podle quantity - vysokÃ© sales majÃ­ vÄ›tÅ¡Ã­ vliv.

---

**ProÄ AI udÄ›lal tuto chybu:**

1. **Literal interpretation:** Prompt Å™ekl "prÅ¯mÄ›rnÃ¡ cena podle regionu" a AI udÄ›lal to co prompt doslova Å™Ã­kÃ¡, ale ignoroval business logic
2. **Pattern matching:** AI vidÄ›l pattern "aggregate â†’ aggregate again" v training datech a aplikoval ho
3. **ChybÄ›jÃ­cÃ­ kontext:** Prompt nespecifikoval "vÃ¡Å¾enÃ½ prÅ¯mÄ›r" nebo "z raw transactions"

**Jak pÅ™edejÃ­t pÅ™Ã­Å¡tÄ›:**

```
Å patnÃ½ prompt:
"prÅ¯mÄ›rnÃ¡ cena produktu podle regionu"

DobrÃ½ prompt:
"Z tabulky @sales (region, unit_price, quantity) vypoÄÃ­tej prÅ¯mÄ›rnou 
unit_price podle regionu. POZOR: PoÄÃ­tej z raw transakcÃ­, ne average-of-averages.
Pokud relevantnÃ­, vÃ¡Å¾ podle quantity.

VÃ½stup: SQL dotaz s: region, avg_price, total_transactions"
```

---

**ÄŒastÃ© podobnÃ© chyby AI:**

| Chyba | PÅ™Ã­klad | Fix |
|-------|---------|-----|
| Average-of-averages | `AVG(AVG(x))` | PoÄÃ­tej z raw data |
| Duplicate counts | `COUNT(id)` v JOINu | `COUNT(DISTINCT id)` |
| NeoÅ¡etÅ™enÃ© NULLs | `SUM(revenue) / COUNT(*)` | `NULLIF`, `COALESCE` |
| Å patnÃ© JOINy | INNER kde mÄ›l bÃ½t LEFT | Specifikuj v promptu |

---

## CviÄenÃ­ 5: ZÃ¡vÄ›reÄnÃ½ Projekt ğŸ”´

### ZadÃ¡nÃ­

VytvoÅ™ kompletnÃ­ end-to-end analÃ½zu podle jednoho z tÄ›chto scÃ©nÃ¡Å™Å¯. PouÅ¾ij VÅ ECHNY techniky z workshopu.

**ScÃ©nÃ¡Å™ A: Geographic Analysis**
Najdi TOP 10 regionÅ¯ podle revenue pro produktovou kategorii "Cardiovascular" za poslednÃ­ch 6 mÄ›sÃ­cÅ¯. Include year-over-year comparison a identifikuj fastest-growing regions.

**ScÃ©nÃ¡Å™ B: Product Performance**
Analyzuj performance vÅ¡ech produktÅ¯ v kategorii "Diabetes Care". Segmentuj do 4 skupin: Stars, Cash Cows, Question Marks, Dogs (podle revenue a growth rate). Recommendations pro kaÅ¾dou skupinu.

**ScÃ©nÃ¡Å™ C: Seasonal Patterns**
Pro kategorii "Cold & Flu" identifikuj seasonal patterns za poslednÃ­ch 2 roky. Kdy jsou peaks? JakÃ¡ je week-over-week volatilita? Predikuj peak pro pÅ™Ã­Å¡tÃ­ rok.

**CÃ­l:** Demonstrovat vÅ¡echny skills z workshopu

**ÄŒasovÃ½ odhad:** 30 minut

### Deliverables

1. **Databricks Notebook** s:
   - Data exploration (@ mentions, schÃ©ma check)
   - SQL dotazy s komentÃ¡Å™i
   - Vizualizace (min. 1 chart)
   - Validace vÃ½sledkÅ¯

2. **Business Insights** (2-3 bullet points):
   - Co jsi zjistil/a?
   - JakÃ© recommendations?

3. **Prompty** kterÃ© jsi pouÅ¾il/a:
   - UkaÅ¾ jak jsi pouÅ¾Ã­val/a prompt chaining
   - Highlight 1-2 good prompts

### PoÅ¾adavky

âœ… Must have:
- PouÅ¾ij @ mentions
- Min. 2 prompty (exploration + analysis)
- SQL dotaz s aggregacÃ­
- 1 visualization
- Validace (check Å¾e vÃ½sledky dÃ¡vajÃ­ smysl)

âš¡ Nice to have:
- Few-shot learning
- Prompt chaining (3+ kroky)
- Multiple viz
- Statistical insights (YoY growth, volatility, atd.)

---

### NÃ¡povÄ›da k Projektu

<details>
<summary>StrategickÃ½ pÅ™Ã­stup</summary>

**Workflow:**

1. **Planning (5 min):**
   - JakÃ© otÃ¡zky chceÅ¡ zodpovÄ›dÄ›t?
   - JakÃ¡ data potÅ™ebujeÅ¡?
   - JakÃ½ je expected output?

2. **Exploration (5 min):**
   - @ mention tabulky
   - Check schÃ©ma, sample data
   - Validuj Å¾e data obsahujÃ­ co potÅ™ebujeÅ¡

3. **Analysis (15 min):**
   - Prompt chaining - rozdÄ›l na kroky
   - SQL dotazy s validacÃ­ kaÅ¾dÃ©ho kroku
   - Aggregace, JOINy, filtry

4. **Visualization (3 min):**
   - Chart type podle dat (bar, line, scatter)
   - Labels, title, formatting

5. **Insights (2 min):**
   - 2-3 key takeaways
   - Business recommendations

**Tip:** NemusÃ­Å¡ bÃ½t perfektnÃ­. Better done than perfect!

</details>

---

### PÅ™Ã­klad Å˜eÅ¡enÃ­ - ScÃ©nÃ¡Å™ A

(Pro inspiraci - tvoje Å™eÅ¡enÃ­ mÅ¯Å¾e vypadat jinak!)

**Krok 1: Exploration**

```python
# Prompt: "Z @demo_sales_data a @demo_products zjisti:
# 1. JakÃ© produkty patÅ™Ã­ do category 'Cardiovascular'
# 2. Sample 5 Å™Ã¡dkÅ¯ pro tuto kategorii
# 3. Date range dat (MIN a MAX date)"

# AI vygeneruje:
%sql
SELECT p.product_name, COUNT(*) as sales_count
FROM demo_sales_data s
JOIN demo_products p ON s.product_id = p.product_id
WHERE p.category = 'Cardiovascular'
GROUP BY p.product_name
ORDER BY sales_count DESC
LIMIT 10;
```

**Krok 2: Current Period Analysis**

```python
# Prompt: "Pro kategorii 'Cardiovascular' za poslednÃ­ch 6 mÄ›sÃ­cÅ¯
# (2024-07-01 aÅ¾ 2024-12-31):
# Agreguj podle region: total_revenue, total_quantity
# TOP 10 regionÅ¯ podle revenue DESC
# SQL s komentÃ¡Å™i"

# AI vygeneruje SQL... (similar to previous exercises)
```

**Krok 3: YoY Comparison**

```python
# Prompt: "PÅ™idej year-over-year srovnÃ¡nÃ­:
# StejnÃ¡ analÃ½za pro same 6 months v roce 2023
# Calculate YoY growth %
# Identifikuj fastest-growing TOP 3 regions"
```

**Krok 4: Visualization**

```python
# Prompt: "Bar chart s YoY comparison:
# Grouped bars (2023 vs 2024) pro TOP 10 regions"
```

**Krok 5: Insights**

```
Key Findings:
â€¢ Region "Praha" dominates with 15.2M KÄ revenue (+25% YoY)
â€¢ Fastest growth: "Liberec" (+89% YoY) - emerging market opportunity
â€¢ "Ostrava" declined -12% YoY - investigate why (competition? demographics?)

Recommendations:
â€¢ Invest marketing in Liberec (high growth potential)
â€¢ Analyze Ostrava decline - retention campaign?
â€¢ Praha - maintain leadership, upsell opportunities
```

---

## Reflexe a Self-Check

Po dokonÄenÃ­ cviÄenÃ­ si poloÅ¾ tyto otÃ¡zky:

- [ ] **ChÃ¡pu proÄ moje prompty fungovaly/nefungovaly?**
  - Co bych pÅ™Ã­Å¡tÄ› udÄ›lal jinak?
  - KterÃ© komponenty (role, kontext, Ãºkol, formÃ¡t) mi pomohly nejvÃ­c?

- [ ] **DokÃ¡Å¾u validovat AI vÃ½stupy?**
  - Zkontroloval/a jsem business logic?
  - Testoval/a jsem edge cases (NULLs, duplicates)?
  - Spustil/a jsem LIMIT 10 pro quick check?

- [ ] **UmÃ­m aplikovat na jinÃ½ use case?**
  - DokÃ¡Å¾u vzÃ­t template z cviÄenÃ­ a pouÅ¾Ã­t na svÃ¡ data?
  - VÃ­m kdy pouÅ¾Ã­t few-shot learning vs. simple prompt?
  - RozumÃ­m prompt chaining?

- [ ] **ZnÃ¡m common pitfalls?**
  - Average-of-averages
  - Duplicate counts
  - IgnorovÃ¡nÃ­ business rules

**Pokud na nÄ›kterou otÃ¡zku odpovÃ­dÃ¡Å¡ NE:**
- Projdi si znovu student guide
- Zkus cviÄenÃ­ s jinÃ½mi daty
- Diskutuj s lektorem nebo kolegy

---

## BonusovÃ© VÃ½zvy (Pro RychlejÅ¡Ã­ Studenty)

### VÃ½zva A: Cohort Analysis

VytvoÅ™ cohort analÃ½zu zÃ¡kaznÃ­kÅ¯:
- Cohorts podle mÄ›sÃ­ce registrace
- Retention rate po 1, 3, 6, 12 mÄ›sÃ­cÃ­ch
- Heatmap vizualizace

### VÃ½zva B: Sentiment Analysis s AI Functions

PouÅ¾ij `ai_analyze_sentiment()` na customer reviews:
```sql
SELECT
  product_id,
  AVG(CASE WHEN sentiment = 'positive' THEN 1 ELSE 0 END) as positive_rate,
  COUNT(*) as review_count
FROM (
  SELECT
    product_id,
    ai_analyze_sentiment(review_text) as sentiment
  FROM customer_reviews
)
GROUP BY product_id;
```

### VÃ½zva C: Predictive Segmentation

VytvoÅ™ scoring model:
- Features: RFM + demographics + product preferences
- Score 0-100 (likelihood of churn)
- Segment: High risk, Medium risk, Low risk

---

## DalÅ¡Ã­ ProcviÄovÃ¡nÃ­

**ChceÅ¡ vÃ­c?**
- [Databricks SQL Exercises](https://docs.databricks.com/sql/index.html)
- [Kaggle Datasets](https://www.kaggle.com/datasets) - pharmacy/healthcare data
- [Mode Analytics SQL Tutorial](https://mode.com/sql-tutorial/) - intermediate/advanced

**NaÅ¡el/a jsi lepÅ¡Ã­ Å™eÅ¡enÃ­?**
- SdÃ­lej v tÃ½mu!
- PÅ™idej do team prompt library
- Diskutuj na Databricks Community Forum

**PotÅ™ebujeÅ¡ pomoc?**
- Databricks Community: community.databricks.com
- Stack Overflow: tag [databricks]
- Email lektor: [email]

---

**ğŸ‰ Gratuluju Å¾e jsi dokonÄil/a vÅ¡echna cviÄenÃ­! TeÄ jsi ready pouÅ¾Ã­vat AI v prÃ¡ci. ğŸš€**

*CviÄenÃ­ vytvoÅ™ena pro workshop "Copilot & Databricks pro farmaceutickÃ© analÃ½zy" | 2025*
